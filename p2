En este ejercicio, vamos a utilizar datos sobre las carreras de los taxis amarillos de Nueva York en Enero de 2024 (se incluye un documento PDF con el diccionario de los metadatos). Además, se usará otro fichero de datos con información sobre los barrios de NYC. El comando o código utilizado (TODOS los PASOS realizados con Spark en este ejercicio, deben ser resueltos con la API de Dataframe, es decir, no se puede utilizar el método spark.sql para lanzar sentencias SQL) en cada paso debe ser incluído en formato texto en el campo de respuesta de la pregunta de Blackboard como solución final:


- Paso 1: Descargar ficheros de datos (No requiere incluir los comandos en la solución final) y explora el contenido del fichero csv (por ejemplo, con el comando head) 


unix-shell> wget -P /home/bigdata/Descargas/ https://gist.githubusercontent.com/mafernandez-stratio/e4a3d3f0c4790989201b2c4af3daa1a1/raw/a983fcf3d1f7beb274c1c792add8a00bb968a5f2/taxi_zones.json

unix-shell> wget -P /home/bigdata/Descargas/ https://gist.githubusercontent.com/mafernandez-stratio/ffc511cfcc5b1c4b260523127689e24b/raw/6bae4169c89c49adb6ec6c09426af2226290d493/yellow_tripdata_2024-01.csv


- Paso 2: Arranca HDFS y copia el fichero taxi_zones.json a la ruta /data/zones/ de HDFS y el fichero yellow_tripdata_2024-01.csv a la ruta /data/taxis/
start-dfs.sh
hdfs dfs -mkdir -p /data/zones
hdfs dfs -mkdir -p /data/taxis

hdfs dfs -put yellow_tripdata_2024-01.csv  /data/taxis
hdfs dfs -put taxi_zones.json  /data/zones


- Paso 3: Arranca la shell de Spark y genera un Dataframe que cargue los datos sobre taxis albergados en HDFS (incluye las opciones del datasource de csv que estimes oportunas tras haber realizado la exploración del fichero en el paso anterior):
spark-shell

val dfTaxis = spark.read.option("header", "true").option("inferSchema", "true").option("delimiter", ":").option("quote", "'").csv("hdfs://localhost:9000/data/taxis/yellow_tripdata_2024-01.csv")


dfTaxis.show(5)
dfTaxis.printSchema()


   


- Paso 4: ¿Cuántos registros tienen un total_amount menor o igual a 0?

import org.apache.spark.sql.functions._

val registrosInvalidos = dfTaxis.filter($"total_amount" <= 0)
val cantidad = registrosInvalidos.count()
println(s"Número de registros con total_amount <= 0: $cantidad")


- Paso 5: Genera una UDF con el nombre calculateTime que obtenga la diferencia en segundos entre 2 timestamps expresados en nanosegundos. Estos timestamps realmente se tratan en Spark como un Long, por lo que la UDF deberá recibir dos variables de tipo Long (ambos timestamps en nanonsegundos) y devolver un Long (diferencia en segundos):

import org.apache.spark.sql.functions.udf



val calculateTime = udf((start: Long, end: Long) => {
  (end - start) / 1000000000L
})



- Paso 6: Genera un Dataframe partiendo del Dataframe del paso 3 con las siguientes operaciones:

* Nueva columna ride_time resultado de ejecutar la UDF calculateTimeUDF sbre las columnas tpep_pickup_datetime y tpep_dropoff_datetime

* Nueva columna Airport_charge que tenga el valor 0 si el valor de la columna Airport_fee es nulo o el propio valor de Airport_fee en caso contrario

* Filtrar las filas que tengan un total_amount mayor que 0, es decir, descartar aquellas filas cuyo total_amount sea 0 o negativo

* Elimina la columna LocationID del Dataframe resultante


import org.apache.spark.sql.functions._


val dfTaxisFinal = dfTaxis
  .withColumn("pickup_ns", col("tpep_pickup_datetime").cast("long") * 1000000000L)
  .withColumn("dropoff_ns", col("tpep_dropoff_datetime").cast("long") * 1000000000L)
  .withColumn("ride_time", calculateTime(col("pickup_ns"), col("dropoff_ns")))
  .withColumn("Airport_charge", when(col("Airport_fee").isNull, lit(0)).otherwise(col("Airport_fee")))
  .filter(col("total_amount") > 0)
  .drop("PULocationID", "DOLocationID", "pickup_ns", "dropoff_ns")


- Paso 7: Genera un nuevo Dataframe desde 0 que cargue los datos sobre zonas de NYC albergados en HDFS:

val dfZones = spark.read.option("multiline", "true").json("hdfs://localhost:9000/data/zones/taxi_zones.json")


- Paso 8: Genera un Dataframe partiendo del Dataframe del paso 6 con las siguientes operaciones:

* Nueva columna PUborough (correspondiente a la columna borough de zonas) generada de un inner join entre las columnas PULocationID de taxis y LocationID de zonas

* Nueva columna DOborough (correspondiente a la columna borough de zonas) generada de un inner join entre las columnas DOLocationID de taxis y LocationID de zonas

* Elimina la columna LocationID del Dataframe resultante

import org.apache.spark.sql.functions._

// Join para obtener PUborough
val dfWithPU = dfTaxis
  .join(dfZones.withColumnRenamed("LocationID", "PULocationID_zone")
               .withColumnRenamed("borough", "PUborough"),
        dfTaxis("PULocationID") === col("PULocationID_zone"),
        "inner"
  )

// Join para obtener DOborough
val dfWithDO = dfWithPU
  .join(dfZones.withColumnRenamed("LocationID", "DOLocationID_zone")
               .withColumnRenamed("borough", "DOborough"),
        dfWithPU("DOLocationID") === col("DOLocationID_zone"),
        "inner"
  )

// Eliminamos columnas de LocationID temporales del join
val dfTaxisBoroughs = dfWithDO
  .drop("PULocationID_zone", "DOLocationID_zone")



//Lo de antes no esta bien; es lo siguiente

val dfTaxisWithBoroughs = dfTaxis.join(dfZones.withColumnRenamed("LocationID", "PULocationID_zone").withColumnRenamed("borough", "PUborough"),dfTaxis("PULocationID") === col("PULocationID_zone"), "inner").join(dfZones.withColumnRenamed("LocationID", "DOLocationID_zone").withColumnRenamed("borough", "DOborough"),col("DOLocationID") === col("DOLocationID_zone"), "inner").drop("PULocationID_zone", "DOLocationID_zone") // elimina columnas extra



//Verificar

dfTaxisBoroughs.select("PULocationID", "PUborough", "DOLocationID", "DOborough").show(10, truncate = false)
dfTaxisBoroughs.printSchema()



//No se si puede ser algo asi por lo del datafram del paso 6

import org.apache.spark.sql.functions._

// UDF del paso 5
val calculateTime = udf((start: Long, end: Long) => (end - start) / 1000000000L)


// Aplicar transformaciones del paso 6
val dfFinal = dfTaxisWithBoroughs.withColumn("pickup_ns", col("tpep_pickup_datetime").cast("long") * 1000000000L).withColumn("dropoff_ns", col("tpep_dropoff_datetime").cast("long") * 1000000000L).withColumn("ride_time", calculateTime(col("pickup_ns"), col("dropoff_ns"))).withColumn("Airport_charge", when(col("Airport_fee").isNull, lit(0)).otherwise(col("Airport_fee"))).filter(col("total_amount") > 0).drop("pickup_ns", "dropoff_ns") // conservamos LocationIDs si los quieres para trazabilidad



- Paso 9: ¿Cuál es la suma total y cuál es el máximo de la columna total_amount? ¿Cuántas carreras duraron más de 1 hora?

import org.apache.spark.sql.functions._

// 1. Suma total y 2. valor máximo de total_amount
dfFinal.agg(
  sum("total_amount").as("suma_total"),
  max("total_amount").as("maximo_total")
).show()

// 3. Número de carreras con duración mayor a 1 hora
val carrerasLargas = dfFinal.filter(col("ride_time") > 3600).count()
println(s"Número de carreras con duración mayor a 1 hora: $carrerasLargas")



- Paso 10: Calcula la media de trip_distance por PUborough y payment_type ordenados por PUborough y payment_type
//Error y tuve q hacer todo again
val dfZones = spark.read.option("multiline", "false").json("hdfs://localhost:9000/data/zones/taxi_zones.json")
val dfTaxis = spark.read.option("header", "true").option("inferSchema", "true").option("delimiter", ":").option("quote", "'").csv("hdfs://localhost:9000/data/taxis/yellow_tripdata_2024-01.csv")
dfTaxis.select("total_amount", "trip_distance").show(10)
val dfZonesCasted = dfZones.withColumn("LocationID", col("LocationID").cast("int"))

val dfTaxisWithBoroughs = dfTaxis.join(dfZonesCasted.withColumnRenamed("LocationID", "PULocationID_zone").withColumnRenamed("borough", "PUborough"),dfTaxis("PULocationID") === col("PULocationID_zone"), "inner").join(dfZonesCasted.withColumnRenamed("LocationID", "DOLocationID_zone").withColumnRenamed("borough", "DOborough"),col("DOLocationID") === col("DOLocationID_zone"), "inner").drop("PULocationID_zone", "DOLocationID_zone") // elimina columnas extra
dfTaxisWithBoroughs.show(10, truncate = false)
val dfFinal = dfTaxisWithBoroughs.withColumn("pickup_ns", col("tpep_pickup_datetime").cast("long") * 1000000000L).withColumn("dropoff_ns", col("tpep_dropoff_datetime").cast("long") * 1000000000L).withColumn("ride_time", calculateTime(col("pickup_ns"), col("dropoff_ns"))).withColumn("Airport_charge", when(col("Airport_fee").isNull, lit(0)).otherwise(col("Airport_fee"))).filter(col("total_amount") > 0).drop("pickup_ns", "dropoff_ns") // conservamos LocationIDs si los quieres para trazabilidad
dfTaxisWithBoroughs.groupBy("PUborough", "payment_type").agg(avg("trip_distance").alias("avg_trip_distance")).orderBy("PUborough", "payment_type").show()



- Paso 11: Calcula cuántos valores nulos hay por columna


- Paso 12: Obten una lista de nombres de columnas del Dataframe del paso 8 que sean de tipo numéricas (integer, double y long) y calcula la correlación entre estas columnas y la columna total_amount


- Paso 13: Genera un Dataframe que realice cuatro transformaciones con SparK MLlib sobre el Dataframe del paso 8 como paso previo a generar una Regresión Lineal sobre la columna total_amount basada en las columnas trip_distance, tolls_amount, Airport_charge, ride_time, PUborough (NOTA: PUborough incialmente es una columna de tipo String)


- Paso 14: Genera un Dataframe que contenga solo las columnas total_amount y la columna del vector de características calculado en el paso anterior. A partir de este Dataframe, genera un Dataframe de datos de entrenamiento (70%) y un Dataframe de datos de testing (30%).


- Paso 15: ¿Cuántas filas tiene cada Dataframe generado en el paso anterior? Lanza también un comando sobre cada Dataframe para obtener count, mean, stddev, min y max de la columna total_amount


- Paso 16: Genera un modelo y un Dataframe con predicciones de una regresión lineal a partir del Dataframe de entrenamiento. Obten el coeficiente de determinación (R2) y explica si el modelo de regresión lineal es suficientemente preciso en función de dicho valor. 


- Paso 17: Evalúa el modelo obteniendo un resumen (LinearRegressionSummary) de acuerdo a los datos de testing, obten el coeficiente de determinación (R2) y explica si el modelo de regresión lineal es suficientemente preciso en función de dicho valor.



import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, OneHotEncoder}
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.Pipeline
import org.apache.spark.sql.types._

// Paso 11: Valores nulos por columna
val dfPaso11 = dfTaxisWithBoroughs.columns.map(c => dfTaxisWithBoroughs.filter(col(c).isNull).count()).zip(dfTaxisWithBoroughs.columns).toSeq.toDF("null_count", "column")
dfPaso11.show()

// Paso 12: Correlación con total_amount para columnas numéricas
val numericCols = dfTaxisWithBoroughs.schema.fields.collect {
  case StructField(name, DoubleType | IntegerType | LongType, _, _) => name
}.filter(_ != "total_amount")
val dfPaso12 = numericCols.map(c => (c, dfTaxisWithBoroughs.stat.corr(c, "total_amount"))).toSeq.toDF("columna", "correlacion")
dfPaso12.show()

// Paso 13: Transformaciones MLlib
val indexer = new StringIndexer().setInputCol("PUborough").setOutputCol("PUborough_index")
val encoder = new OneHotEncoder().setInputCol("PUborough_index").setOutputCol("PUborough_vec")
val assembler = new VectorAssembler()
  .setInputCols(Array("trip_distance", "tolls_amount", "Airport_fee", "ride_time", "PUborough_vec"))
  .setOutputCol("features")

val pipeline = new Pipeline().setStages(Array(indexer, encoder, assembler))
val dfPaso13 = pipeline.fit(dfTaxisWithBoroughs).transform(dfTaxisWithBoroughs)
dfPaso13.select("features", "total_amount").show(false)

// Paso 14: Split train/test
val dfPaso14 = dfPaso13.select("features", "total_amount")
val Array(dfTrain, dfTest) = dfPaso14.randomSplit(Array(0.7, 0.3), seed = 42)

// Paso 15: Estadísticas
println(s"Entrenamiento: ${dfTrain.count()} filas")
dfTrain.describe("total_amount").show()
println(s"Testing: ${dfTest.count()} filas")
dfTest.describe("total_amount").show()

// Paso 16: Modelo de regresión lineal
val lr = new LinearRegression().setLabelCol("total_amount").setFeaturesCol("features")
val modelo = lr.fit(dfTrain)
val dfPredicciones = modelo.transform(dfTrain)
val r2Train = modelo.summary.r2
println(s"R2 entrenamiento: $r2Train")

// Paso 17: Evaluación en test
val evaluacion = modelo.evaluate(dfTest)
val r2Test = evaluacion.r2
println(s"R2 testing: $r2Test")
